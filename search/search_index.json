{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"tEKS \u00b6 Terraform/Terragrunt Contributing Requirements Terragrunt Quickstart Main purposes What you get Curated Features Bottlerocket support AWS Session Manager by default From and to Zero scaling with EKS Managed Node Groups Automatic dependencies upgrade Enforced security Out of the box logging Out of the box monitoring Long term storage with Thanos Support for ARM instances Helm v3 provider Other and not limited to Always up to date Requirements Pre-commit ASDF Enabling plugins Installing tools Examples Additional infrastructure blocks Branches License tEKS is a set of Terraform / Terragrunt modules designed to get you everything you need to run a production EKS cluster on AWS. It ships with sensible defaults, and add a lot of common addons with their configurations that work out of the box. This is our opinionated view of what a well structred infrastructure as code repository should look like. the v5 and further version of this project have been completely revamp and now offer a skeleton to use as a base for your infrastructure projects around EKS. All the modules have been moved outside this repository and get their own versioning. The old README is accessible here Terraform implementation will not be maintained anymore because of time, and mostly because it has become quite difficult to get feature parity with Terragrunt. Archive branch is available here Terraform/Terragrunt \u00b6 Terragrunt implementation is available in the terragrunt folder. Contributing \u00b6 Contribution are welcome, as well as issues, we are usually quite reactive. If you need more support for your project, do not hesitate to reach us directly . Requirements \u00b6 Terragrunt \u00b6 Terraform Terragrunt Quickstart \u00b6 Quickstart guide is available here or on the official documentation website Main purposes \u00b6 The main goal of this project is to glue together commonly used tooling with Kubernetes/EKS and to get from an AWS Account to a production cluster with everything you need without any manual configuration. What you get \u00b6 A production cluster all defined in IaaC with Terraform/Terragrunt: AWS VPC if needed based on terraform-aws-vpc EKS cluster base on terraform-aws-eks Kubernetes addons based on terraform-kubernetes-addons : provides various addons that are often used on Kubernetes and specifically on EKS. This module is currated by Particule and well maintained. Everything is tied together with Terragrunt and allows you to deploy a multi cluster architecture in a matter of minutes. Curated Features \u00b6 The additional features are provided by tEKS here as well as our curated addons module which support a bunch of various configuration. Bottlerocket support \u00b6 Bottlerocket OS is available for node groups (see example here ). Bottle rocket is a container centric OS with less attack surface and no default shell. AWS Session Manager by default \u00b6 All the instances (Bottlerocket or Amazon Linux) are registered with AWS Session Manager . No SSH keys or SSH access is open on instances. Shell access on every instance can be given with SSM for added security. aws ssm start-session --target INSTANCE_ID From and to Zero scaling with EKS Managed Node Groups \u00b6 tEKS support scaling to and from 0, even with using well know Kubernetes labels , there are a number of ongoing issues for support of EKS Managed node groups with Cluster Autoscaler . Thanks to automatic ASG tagging , tEKS adds the necessary tags on autoscaling group to balance similar node groups and allow you to scale to and from 0 and even to use well know labels such as node.kubernetes.io/instance-type or topology.kubernetes.io/zone . The logic can be extended to support other well known labels. Automatic dependencies upgrade \u00b6 We are using renovate to automatically open PR with the latest dependencies update (Terraform modules upgrade) so you never miss an upgrade and are alwasy up to date with the latest features. Enforced security \u00b6 Encryption by default for root volume on instances with Custom KMS Key AWS EBS CSI volumes encrypted by default with Custom KMS Key No IAM credentials on instances, everything is enforced with IRSA . Each addons is deployed in it's own namespace with sensible default network policies. Calico Tigera Operator for network policy. PSP are enabled but not enforced because of depreciation. Out of the box logging \u00b6 Three stacks are supported: * AWS for Fluent Bit : Forward containers logs to Cloudwatch Logs * Grafana Loki : Uses Promtail to forward logs to Loki . Grafana or a tEKS supported monitoring stack (see below) is necessary to display logs. Out of the box monitoring \u00b6 Prometheus Operator with defaults dashboards Addons that support metrics are enable along with their serviceMonitor Custom grafana dashboard are available by default Two stacks are supported: * Victoria Metrics Stack : Victoria Metrics is a Prometheus alertnative, compatible with prometheus CRDs * Kube Prometheus Stack : Classic Prometheus Monitoring Long term storage with Thanos \u00b6 With Prometheus, tEKS includes Thanos by default. Thanos uses S3 to store and query metrics, offering long term storage without the costs. For more information check out our article on the CNCF Blog Support for ARM instances \u00b6 With either Amazon Linux or BottleRocket, you can use a mix of ARM and AMD64 instances. Check out our example Helm v3 provider \u00b6 All addons support Helm v3 configuration All charts are easily customizable Other and not limited to \u00b6 priorityClasses for addons and critical addons lot of manual stuff have been automated under the hood Always up to date \u00b6 We always support the latest modules and features for our addons module . Our cutting edges addons include (not limited to): * AWS EBS CSI Drivers : Support for Volume encryption by default, snapshot, etc * AWS EFS CSI Drivers : Use AWS NFS shares. * Secret Store CSI Driver : load secret from Secret Managers with aws-secret-store-csi-driver driver * Linkerd2 or Certificate Manager CSI for mTLS Requirements \u00b6 Terragrunt is not a hard requirement but all the modules are tested with Terragrunt. Terraform Terragrunt kubectl helm Pre-commit \u00b6 This repository use pre-commit hooks, please see this on how to setup tooling ASDF \u00b6 ASDF is a package manager which is great for managing cloud native tooling. More info here (eg. French). Enabling plugins \u00b6 for p in $(cut -d \" \" .tool-versions -f1); do asdf plugin add $p; done Installing tools \u00b6 asdf install Examples \u00b6 terragrunt/live folder provides an opinionated directory structure for a production environment. Additional infrastructure blocks \u00b6 If you wish to extend your infrastructure you can pick up additional modules on the particuleio github page . Some modules can also be found on the clusterfrak-dynamics github page . Branches \u00b6 main : Backward incompatible with v1.X but compatible with v2.X, releases bumped to v3.X because a lot has changed. release-1.X : Compatible with Terraform < 0.12 and Terragrunt < 0.19. Be sure to target the same modules version. release-2.X : Compatible with Terraform >= 0.12 and Terragrunt >= 0.19. Be sure to target the same modules version. License \u00b6","title":"Overview"},{"location":"#teks","text":"Terraform/Terragrunt Contributing Requirements Terragrunt Quickstart Main purposes What you get Curated Features Bottlerocket support AWS Session Manager by default From and to Zero scaling with EKS Managed Node Groups Automatic dependencies upgrade Enforced security Out of the box logging Out of the box monitoring Long term storage with Thanos Support for ARM instances Helm v3 provider Other and not limited to Always up to date Requirements Pre-commit ASDF Enabling plugins Installing tools Examples Additional infrastructure blocks Branches License tEKS is a set of Terraform / Terragrunt modules designed to get you everything you need to run a production EKS cluster on AWS. It ships with sensible defaults, and add a lot of common addons with their configurations that work out of the box. This is our opinionated view of what a well structred infrastructure as code repository should look like. the v5 and further version of this project have been completely revamp and now offer a skeleton to use as a base for your infrastructure projects around EKS. All the modules have been moved outside this repository and get their own versioning. The old README is accessible here Terraform implementation will not be maintained anymore because of time, and mostly because it has become quite difficult to get feature parity with Terragrunt. Archive branch is available here","title":"tEKS"},{"location":"#terraformterragrunt","text":"Terragrunt implementation is available in the terragrunt folder.","title":"Terraform/Terragrunt"},{"location":"#contributing","text":"Contribution are welcome, as well as issues, we are usually quite reactive. If you need more support for your project, do not hesitate to reach us directly .","title":"Contributing"},{"location":"#requirements","text":"","title":"Requirements"},{"location":"#terragrunt","text":"Terraform Terragrunt","title":"Terragrunt"},{"location":"#quickstart","text":"Quickstart guide is available here or on the official documentation website","title":"Quickstart"},{"location":"#main-purposes","text":"The main goal of this project is to glue together commonly used tooling with Kubernetes/EKS and to get from an AWS Account to a production cluster with everything you need without any manual configuration.","title":"Main purposes"},{"location":"#what-you-get","text":"A production cluster all defined in IaaC with Terraform/Terragrunt: AWS VPC if needed based on terraform-aws-vpc EKS cluster base on terraform-aws-eks Kubernetes addons based on terraform-kubernetes-addons : provides various addons that are often used on Kubernetes and specifically on EKS. This module is currated by Particule and well maintained. Everything is tied together with Terragrunt and allows you to deploy a multi cluster architecture in a matter of minutes.","title":"What you get"},{"location":"#curated-features","text":"The additional features are provided by tEKS here as well as our curated addons module which support a bunch of various configuration.","title":"Curated Features"},{"location":"#bottlerocket-support","text":"Bottlerocket OS is available for node groups (see example here ). Bottle rocket is a container centric OS with less attack surface and no default shell.","title":"Bottlerocket support"},{"location":"#aws-session-manager-by-default","text":"All the instances (Bottlerocket or Amazon Linux) are registered with AWS Session Manager . No SSH keys or SSH access is open on instances. Shell access on every instance can be given with SSM for added security. aws ssm start-session --target INSTANCE_ID","title":"AWS Session Manager by default"},{"location":"#from-and-to-zero-scaling-with-eks-managed-node-groups","text":"tEKS support scaling to and from 0, even with using well know Kubernetes labels , there are a number of ongoing issues for support of EKS Managed node groups with Cluster Autoscaler . Thanks to automatic ASG tagging , tEKS adds the necessary tags on autoscaling group to balance similar node groups and allow you to scale to and from 0 and even to use well know labels such as node.kubernetes.io/instance-type or topology.kubernetes.io/zone . The logic can be extended to support other well known labels.","title":"From and to Zero scaling with EKS Managed Node Groups"},{"location":"#automatic-dependencies-upgrade","text":"We are using renovate to automatically open PR with the latest dependencies update (Terraform modules upgrade) so you never miss an upgrade and are alwasy up to date with the latest features.","title":"Automatic dependencies upgrade"},{"location":"#enforced-security","text":"Encryption by default for root volume on instances with Custom KMS Key AWS EBS CSI volumes encrypted by default with Custom KMS Key No IAM credentials on instances, everything is enforced with IRSA . Each addons is deployed in it's own namespace with sensible default network policies. Calico Tigera Operator for network policy. PSP are enabled but not enforced because of depreciation.","title":"Enforced security"},{"location":"#out-of-the-box-logging","text":"Three stacks are supported: * AWS for Fluent Bit : Forward containers logs to Cloudwatch Logs * Grafana Loki : Uses Promtail to forward logs to Loki . Grafana or a tEKS supported monitoring stack (see below) is necessary to display logs.","title":"Out of the box logging"},{"location":"#out-of-the-box-monitoring","text":"Prometheus Operator with defaults dashboards Addons that support metrics are enable along with their serviceMonitor Custom grafana dashboard are available by default Two stacks are supported: * Victoria Metrics Stack : Victoria Metrics is a Prometheus alertnative, compatible with prometheus CRDs * Kube Prometheus Stack : Classic Prometheus Monitoring","title":"Out of the box monitoring"},{"location":"#long-term-storage-with-thanos","text":"With Prometheus, tEKS includes Thanos by default. Thanos uses S3 to store and query metrics, offering long term storage without the costs. For more information check out our article on the CNCF Blog","title":"Long term storage with Thanos"},{"location":"#support-for-arm-instances","text":"With either Amazon Linux or BottleRocket, you can use a mix of ARM and AMD64 instances. Check out our example","title":"Support for ARM instances"},{"location":"#helm-v3-provider","text":"All addons support Helm v3 configuration All charts are easily customizable","title":"Helm v3 provider"},{"location":"#other-and-not-limited-to","text":"priorityClasses for addons and critical addons lot of manual stuff have been automated under the hood","title":"Other and not limited to"},{"location":"#always-up-to-date","text":"We always support the latest modules and features for our addons module . Our cutting edges addons include (not limited to): * AWS EBS CSI Drivers : Support for Volume encryption by default, snapshot, etc * AWS EFS CSI Drivers : Use AWS NFS shares. * Secret Store CSI Driver : load secret from Secret Managers with aws-secret-store-csi-driver driver * Linkerd2 or Certificate Manager CSI for mTLS","title":"Always up to date"},{"location":"#requirements_1","text":"Terragrunt is not a hard requirement but all the modules are tested with Terragrunt. Terraform Terragrunt kubectl helm","title":"Requirements"},{"location":"#pre-commit","text":"This repository use pre-commit hooks, please see this on how to setup tooling","title":"Pre-commit"},{"location":"#asdf","text":"ASDF is a package manager which is great for managing cloud native tooling. More info here (eg. French).","title":"ASDF"},{"location":"#enabling-plugins","text":"for p in $(cut -d \" \" .tool-versions -f1); do asdf plugin add $p; done","title":"Enabling plugins"},{"location":"#installing-tools","text":"asdf install","title":"Installing tools"},{"location":"#examples","text":"terragrunt/live folder provides an opinionated directory structure for a production environment.","title":"Examples"},{"location":"#additional-infrastructure-blocks","text":"If you wish to extend your infrastructure you can pick up additional modules on the particuleio github page . Some modules can also be found on the clusterfrak-dynamics github page .","title":"Additional infrastructure blocks"},{"location":"#branches","text":"main : Backward incompatible with v1.X but compatible with v2.X, releases bumped to v3.X because a lot has changed. release-1.X : Compatible with Terraform < 0.12 and Terragrunt < 0.19. Be sure to target the same modules version. release-2.X : Compatible with Terraform >= 0.12 and Terragrunt >= 0.19. Be sure to target the same modules version.","title":"Branches"},{"location":"#license","text":"","title":"License"},{"location":"quickstart/","text":"Quickstart \u00b6 Cloud Requirements \u00b6 At least one AWS Account with AdministratorAccess AWS CLI configured with the account you want to deploy into An AWS route53 domain name if you want default Ingresses to just work. box. It is fine without it but External DNS and Cert Manager won't work out of the box Dependencies \u00b6 Dependencies can be found in .tools-version this file is compatible with asdf which is not a hard requirement but our way of managing required tooling. Enabling plugins \u00b6 for p in $(cut -d \" \" .tool-versions -f1); do asdf plugin add $p; done Installing tools \u00b6 asdf install Create repository structure \u00b6 Clone the template on Github In ./terragrunt/live/global_values adapt to your requirements --- # Your AWS Account ID aws_account_id: 161285725140 # Prefix to be added to created resources prefix: pio-teks # AWS S3 bucket region where Terraform will store state tf_state_bucket_region: eu-west-1 # Github username or organization, this can be used by Flux2 to auto configure # Github bootstrap github_owner: particuleio In ./terragrunt/live/production/env_values.yaml adapt to your requirements, it is also possible to override variables defined in global_values.yaml here, for example when using different AWS account per environment. Here we will use only one AWS account and deploy the production environment. --- # Environment name, normally equal to folder name, here it is production by default env: production # Default domain name that will be used by default ingress resources, use a registered Route53 domain in the AWS Account default_domain_name: clusterfrak-dynamics.io In terragrunt/live/production/eu-west-1/region_values.yaml there is nothing to change if you want to use the example region ( eu-west-1 ), if you want to use another region, just rename the folder, for example us-east-1 and then edit region_values.yaml to suit your need. --- aws_region: eu-west-1 In terragrunt/live/production/eu-west-1/clusters/demo/component_values.yaml , name will be used to compute full cluster name, the default is $PREFIX-$ENV_$NAME which is defined here . It is of course possible to override default variable inside the respective terragrunt.hcl files You can edit each modules individually inside terragrunt/live/production/eu-west-1/clusters/demo . For official modules, please refer to their respective documentations. For eks-addons you can check the module here . Configure Flux2 Gitops in terragrunt/live/production/eu-west-1/clusters/demo/eks-addons/terragrunt.hcl or disable it if needed, you will need a GITHUB_TOKEN available from you terminal. Also to configure it according the your repository name. # For this to work: # * GITHUB_TOKEN should be set flux2 = { enabled = true target_path = \"gitops/clusters/${include.root.locals.merged.env}/${include.root.locals.merged.name}\" github_url = \"ssh://git@github.com/${include.root.locals.merged.github_owner}/teks\" repository = \"teks\" branch = \"main\" repository_visibility = \"public\" version = \"v0.25.3\" auto_image_update = true } Make sure you AWS credential are correctly loaded inside your terminal, then from the terragrunt/live/production/eu-west-1/clusters/demo . terragrunt run-all apply INFO[0000] The stack at /home/klefevre/git/archifleks/teks-quickstart/terragrunt/live/production/eu-west-1/clusters/demo will be processed in the following order for command apply: Group 1 - Module /home/klefevre/git/archifleks/teks-quickstart/terragrunt/live/production/eu-west-1/clusters/demo/encryption-config - Module /home/klefevre/git/archifleks/teks-quickstart/terragrunt/live/production/eu-west-1/clusters/demo/vpc Group 2 - Module /home/klefevre/git/archifleks/teks-quickstart/terragrunt/live/production/eu-west-1/clusters/demo/eks - Module /home/klefevre/git/archifleks/teks-quickstart/terragrunt/live/production/eu-west-1/clusters/demo/vpc-endpoints Group 3 - Module /home/klefevre/git/archifleks/teks-quickstart/terragrunt/live/production/eu-west-1/clusters/demo/aws-auth - Module /home/klefevre/git/archifleks/teks-quickstart/terragrunt/live/production/eu-west-1/clusters/demo/eks-addons-critical Group 4 - Module /home/klefevre/git/archifleks/teks-quickstart/terragrunt/live/production/eu-west-1/clusters/demo/eks-addons Load Kubeconfig, you still need to have the AWS CLI loaded and configure with the right account export KUBECONFIG=$PWD/eks/kubeconfig Check out ingress objects k get ingress --all-namespaces NAMESPACE NAME CLASS HOSTS ADDRESS PORTS AGE monitoring kube-prometheus-stack-grafana nginx telemetry.clusterfrak-dynamics.io k8s-ingressn-ingressn-d192ac60af-c080dd921f212013.elb.eu-west-1.amazonaws.com 80, 443 12m Log into Grafana. From the eks-addons folder terragrunt output grafana_password \"PASSWORD\" Use the cluster to do stuff you normally do on a Kubernetes Cluster To destroy everything simply run terragrunt run-all destroy --terragrunt-exclude-dir=aws-auth from the eu-west-1/clusters/demo folder. there is an issue with flux 2 namespace not terminating correctly because CRDs are deleted before namespace is terminated. To unstuck flux-system namespace deletion, you can run the following command: kubectl get namespace \"flux-system\" -o json | tr -d \"\\n\" | sed \"s/\\\"finalizers\\\": \\[[^]]\\+\\]/\\\"finalizers\\\": []/\" | kubectl replace --raw /api/v1/namespaces/flux-system/finalize -f - Verify everything is deleted on AWS console (I just did not want the quickstart to end on an odd number)","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"quickstart/#cloud-requirements","text":"At least one AWS Account with AdministratorAccess AWS CLI configured with the account you want to deploy into An AWS route53 domain name if you want default Ingresses to just work. box. It is fine without it but External DNS and Cert Manager won't work out of the box","title":"Cloud Requirements"},{"location":"quickstart/#dependencies","text":"Dependencies can be found in .tools-version this file is compatible with asdf which is not a hard requirement but our way of managing required tooling.","title":"Dependencies"},{"location":"quickstart/#enabling-plugins","text":"for p in $(cut -d \" \" .tool-versions -f1); do asdf plugin add $p; done","title":"Enabling plugins"},{"location":"quickstart/#installing-tools","text":"asdf install","title":"Installing tools"},{"location":"quickstart/#create-repository-structure","text":"Clone the template on Github In ./terragrunt/live/global_values adapt to your requirements --- # Your AWS Account ID aws_account_id: 161285725140 # Prefix to be added to created resources prefix: pio-teks # AWS S3 bucket region where Terraform will store state tf_state_bucket_region: eu-west-1 # Github username or organization, this can be used by Flux2 to auto configure # Github bootstrap github_owner: particuleio In ./terragrunt/live/production/env_values.yaml adapt to your requirements, it is also possible to override variables defined in global_values.yaml here, for example when using different AWS account per environment. Here we will use only one AWS account and deploy the production environment. --- # Environment name, normally equal to folder name, here it is production by default env: production # Default domain name that will be used by default ingress resources, use a registered Route53 domain in the AWS Account default_domain_name: clusterfrak-dynamics.io In terragrunt/live/production/eu-west-1/region_values.yaml there is nothing to change if you want to use the example region ( eu-west-1 ), if you want to use another region, just rename the folder, for example us-east-1 and then edit region_values.yaml to suit your need. --- aws_region: eu-west-1 In terragrunt/live/production/eu-west-1/clusters/demo/component_values.yaml , name will be used to compute full cluster name, the default is $PREFIX-$ENV_$NAME which is defined here . It is of course possible to override default variable inside the respective terragrunt.hcl files You can edit each modules individually inside terragrunt/live/production/eu-west-1/clusters/demo . For official modules, please refer to their respective documentations. For eks-addons you can check the module here . Configure Flux2 Gitops in terragrunt/live/production/eu-west-1/clusters/demo/eks-addons/terragrunt.hcl or disable it if needed, you will need a GITHUB_TOKEN available from you terminal. Also to configure it according the your repository name. # For this to work: # * GITHUB_TOKEN should be set flux2 = { enabled = true target_path = \"gitops/clusters/${include.root.locals.merged.env}/${include.root.locals.merged.name}\" github_url = \"ssh://git@github.com/${include.root.locals.merged.github_owner}/teks\" repository = \"teks\" branch = \"main\" repository_visibility = \"public\" version = \"v0.25.3\" auto_image_update = true } Make sure you AWS credential are correctly loaded inside your terminal, then from the terragrunt/live/production/eu-west-1/clusters/demo . terragrunt run-all apply INFO[0000] The stack at /home/klefevre/git/archifleks/teks-quickstart/terragrunt/live/production/eu-west-1/clusters/demo will be processed in the following order for command apply: Group 1 - Module /home/klefevre/git/archifleks/teks-quickstart/terragrunt/live/production/eu-west-1/clusters/demo/encryption-config - Module /home/klefevre/git/archifleks/teks-quickstart/terragrunt/live/production/eu-west-1/clusters/demo/vpc Group 2 - Module /home/klefevre/git/archifleks/teks-quickstart/terragrunt/live/production/eu-west-1/clusters/demo/eks - Module /home/klefevre/git/archifleks/teks-quickstart/terragrunt/live/production/eu-west-1/clusters/demo/vpc-endpoints Group 3 - Module /home/klefevre/git/archifleks/teks-quickstart/terragrunt/live/production/eu-west-1/clusters/demo/aws-auth - Module /home/klefevre/git/archifleks/teks-quickstart/terragrunt/live/production/eu-west-1/clusters/demo/eks-addons-critical Group 4 - Module /home/klefevre/git/archifleks/teks-quickstart/terragrunt/live/production/eu-west-1/clusters/demo/eks-addons Load Kubeconfig, you still need to have the AWS CLI loaded and configure with the right account export KUBECONFIG=$PWD/eks/kubeconfig Check out ingress objects k get ingress --all-namespaces NAMESPACE NAME CLASS HOSTS ADDRESS PORTS AGE monitoring kube-prometheus-stack-grafana nginx telemetry.clusterfrak-dynamics.io k8s-ingressn-ingressn-d192ac60af-c080dd921f212013.elb.eu-west-1.amazonaws.com 80, 443 12m Log into Grafana. From the eks-addons folder terragrunt output grafana_password \"PASSWORD\" Use the cluster to do stuff you normally do on a Kubernetes Cluster To destroy everything simply run terragrunt run-all destroy --terragrunt-exclude-dir=aws-auth from the eu-west-1/clusters/demo folder. there is an issue with flux 2 namespace not terminating correctly because CRDs are deleted before namespace is terminated. To unstuck flux-system namespace deletion, you can run the following command: kubectl get namespace \"flux-system\" -o json | tr -d \"\\n\" | sed \"s/\\\"finalizers\\\": \\[[^]]\\+\\]/\\\"finalizers\\\": []/\" | kubectl replace --raw /api/v1/namespaces/flux-system/finalize -f - Verify everything is deleted on AWS console (I just did not want the quickstart to end on an odd number)","title":"Create repository structure"},{"location":"user-guides/eks-addons/","text":"EKS addons module \u00b6 terraform-kubernetes-addons:aws is a custom module maintained here and provides: helm v3 charts manifests operators For commonly used addons one Kubernetes and most specifically with EKS. The configuration is curated to be tightly integrated with AWS and EKS. Customization \u00b6 All the configuration is done in eks-addons/terragrunt.hcl . depe n de n cies { pa t hs = [ \"../eks-addons-critical\" ] } i n clude \"root\" { pa t h = f i n d_i n _pare nt _ f olders() expose = true merge_s trate gy = \"deep\" } i n clude \"vpc\" { pa t h = \"../../../../../../dependency-blocks/vpc.hcl\" expose = true merge_s trate gy = \"deep\" } i n clude \"eks\" { pa t h = \"../../../../../../dependency-blocks/eks.hcl\" expose = true merge_s trate gy = \"deep\" } terraf orm { source = \"github.com/particuleio/terraform-kubernetes-addons.git//modules/aws?ref=v5.0.0\" } ge nerate \"provider-local\" { pa t h = \"provider-local.tf\" i f _exis ts = \"overwrite\" co ntents = f ile( \"../../../../../../provider-config/eks-addons/eks-addons.tf\" ) } ge nerate \"provider-github\" { pa t h = \"provider-github.tf\" i f _exis ts = \"overwrite_terragrunt\" co ntents = << -E OF provider \"github\" { ow ner = \"${include.root.locals.merged.github_owner}\" } EOF } i n pu ts = { priori t y - class = { na me = base na me(ge t _ terra gru nt _dir()) } priori t y - class - ds = { na me = \"${basename(get_terragrunt_dir())}-ds\" } clus ter - na me = depe n de n cy.eks.ou t pu ts .clus ter _id ta gs = merge( i n clude.roo t .locals.cus t om_ ta gs ) eks = { \"cluster_oidc_issuer_url\" = depe n de n cy.eks.ou t pu ts .clus ter _oidc_issuer_url } cer t - ma na ger = { e na bled = true acme_h tt p 01 _e na bled = true acme_d ns 01 _e na bled = true acme_h tt p 01 _i n gress_class = \"nginx\" ex tra _values = << -E XTRA_VALUES i n gressShim : de fault IssuerName : le tsen cryp t de fault IssuerKi n d : Clus ter Issuer de fault IssuerGroup : cer t - ma na ger.io EXTRA_VALUES } clus ter - au t oscaler = { e na bled = true versio n = \"v1.21.2\" ex tra _values = << -E XTRA_VALUES ex tra Args : scale - dow n - u t iliza t io n - t hreshold : 0.7 EXTRA_VALUES } ex ternal - d ns = { ex ternal - d ns = { e na bled = true } } # For t his t o work : # * GITHUB_TOKEN should be se t flu x 2 = { e na bled = true tar ge t _pa t h = \"gitops/clusters/${include.root.locals.merged.env}/${include.root.locals.merged.name}\" gi t hub_url = \"ssh://git@github.com/particuleio/teks\" reposi t ory = \"teks\" bra n ch = \"flux\" reposi t ory_visibili t y = \"public\" versio n = \"v0.27.1\" au t o_image_upda te = true } i n gress - n gi n x = { e na bled = true use_ nl b_ip = true allowed_cidrs = depe n de n cy.vpc.ou t pu ts .priva te _sub nets _cidr_blocks ex tra _values = << -E XTRA_VALUES co ntr oller : i n gressClassResource : e na bled : true de fault : true replicaCou nt : 2 mi n Available : 1 ki n d : \"Deployment\" resources : reques ts : cpu : 100 m memory : 64 Mi EXTRA_VALUES } kube - prome t heus - s ta ck = { e na bled = true allowed_cidrs = depe n de n cy.vpc.ou t pu ts .priva te _sub nets _cidr_blocks t ha n os_sidecar_e na bled = true t ha n os_bucke t _ f orce_des tr oy = true ex tra _values = << -E XTRA_VALUES gra fana : image : ta g : 8.3.4 deployme nt S trate gy : t ype : Recrea te i n gress : a nn o tat io ns : kuber netes .io/ tls - acme : \"true\" i n gressClassName : n gi n x e na bled : true hos ts : - tele me tr y.$ { i n clude.roo t .locals.merged.de fault _domai n _ na me } tls : - secre t Name : $ { i n clude.roo t .locals.merged.de fault _domai n _ na me } hos ts : - tele me tr y.$ { i n clude.roo t .locals.merged.de fault _domai n _ na me } persis ten ce : e na bled : true accessModes : - ReadWri te O n ce size : 1 Gi prome t heus : prome t heusSpec : n odeSelec t or : kuber netes .io/arch : amd 64 scrapeI nter val : 60 s re tent io n : 2 d re tent io n Size : \"10GB\" ruleSelec t orNilUsesHelmValues : false serviceMo n i t orSelec t orNilUsesHelmValues : false podMo n i t orSelec t orNilUsesHelmValues : false probeSelec t orNilUsesHelmValues : false s t orageSpec : volumeClaimTempla te : spec : accessModes : [ \"ReadWriteOnce\" ] resources : reques ts : s t orage : 10 Gi resources : reques ts : cpu : 1 memory : 2 Gi limi ts : cpu : 2 memory : 2 Gi EXTRA_VALUES } loki - s ta ck = { e na bled = true bucke t _ f orce_des tr oy = true ex tra _values = << - VALUES resources : reques ts : cpu : 1 memory : 2 Gi limi ts : cpu : 2 memory : 4 Gi co nf ig : limi ts _co nf ig : i n ges t io n _ra te _mb : 320 i n ges t io n _burs t _size_mb : 512 max_s trea ms_per_user : 100000 chu n k_s t ore_co nf ig : max_look_back_period : 2160 h ta ble_ma na ger : re tent io n _dele tes _e na bled : true re tent io n _period : 2160 h i n gress : e na bled : true a nn o tat io ns : kuber netes .io/ tls - acme : \"true\" n gi n x.i n gress.kuber netes .io/au t h - tls - veri f y - clie nt : \"on\" n gi n x.i n gress.kuber netes .io/au t h - tls - secre t : \"telemetry/loki-ca\" hos ts : - hos t : logz.$ { i n clude.roo t .locals.merged.de fault _domai n _ na me } pa t hs : [ \"/\" ] tls : - secre t Name : logz.$ { i n clude.roo t .locals.merged.de fault _domai n _ na me } hos ts : - logz.$ { i n clude.roo t .locals.merged.de fault _domai n _ na me } VALUES bucke t _li fe cycle_rule = [ { id = \"log\" e na bled = true trans i t io n = [ { days = 14 s t orage_class = \"INTELLIGENT_TIERING\" }, ] expira t io n = { days = 30 } }, ] } prom ta il = { e na bled = true wai t = false } t ha n os = { e na bled = true bucke t _ f orce_des tr oy = true # Wai t i n g f or ARM suppor t h tt ps : //gi t hub.com/bi tna mi/char ts /issues/ 7305 ex tra _values = << -E XTRA_VALUES query : n odeSelec t or : kuber netes .io/arch : amd 64 queryFro nten d : n odeSelec t or : kuber netes .io/arch : amd 64 bucke t web : n odeSelec t or : kuber netes .io/arch : amd 64 compac t or : n odeSelec t or : kuber netes .io/arch : amd 64 s t orega te way : n odeSelec t or : kuber netes .io/arch : amd 64 ruler : n odeSelec t or : kuber netes .io/arch : amd 64 receive : n odeSelec t or : kuber netes .io/arch : amd 64 receiveDis tr ibu t or : n odeSelec t or : kuber netes .io/arch : amd 64 EXTRA_VALUES } } Default charts values \u00b6 Some values are defined by default directly into the module. These can off course be overridden and or merged/replaced. You can find the defaults values in the upstream module . Eg. default values for cluster-autoscaler are in cluster-autoscaler.tf . Overriding Helm provider values \u00b6 Helm provider have defaults values defined here : helm_de faults _de faults = { a t omic = false clea nu p_o n _ fa il = false depe n de n cy_upda te = false disable_crd_hooks = false disable_webhooks = false f orce_upda te = false recrea te _pods = false re n der_subchar t _ n o tes = true replace = false rese t _values = false reuse_values = false skip_crds = false t imeou t = 3600 veri f y = false wai t = true ex tra _values = \"\" } These can be overridden globally with the helm_defaults input variable or can be overridden per chart in terragrunt.hcl : helm_de faults = { replace = true veri f y = true t imeou t = 300 } clus ter _au t oscaler = { crea te _iam_resources_irsa = true iam_policy_override = \"\" versio n = \"v1.14.7\" char t _versio n = \"6.4.0\" e na bled = true de fault _ net work_policy = true clus ter _ na me = depe n de n cy.eks.ou t pu ts .clus ter _id t imeou t = 3600 <= here you ca n add a n y helm provider override } Overriding charts values.yaml \u00b6 It is possible to add or override values per charts. Helm provider use the same merge logic as Helm so you can basically rewrite the whole values.yaml if needed. Each chart has a extra_values variable where you can specify custom values. flu x = { crea te _iam_resources_irsa = true versio n = \"1.18.0\" char t _versio n = \"1.2.0\" e na bled = false de fault _ net work_policy = true ex tra _values = <<EXTRA_VALUES gi t : url : \"ssh://git@gitlab.com/myrepo/gitops-${local.env}.git\" pollI nter val : \"2m\" rbac : crea te : false regis tr y : au t oma t io n I nter val : \"2m\" EXTRA_VALUES } There are some examples in the terragrunt.hcl file. Not all the variables available are present. If you want a full list of variable, you can find them in the upstream module . For example for cluster-autoscaler you can see the default here .","title":"EKS Addons"},{"location":"user-guides/eks-addons/#eks-addons-module","text":"terraform-kubernetes-addons:aws is a custom module maintained here and provides: helm v3 charts manifests operators For commonly used addons one Kubernetes and most specifically with EKS. The configuration is curated to be tightly integrated with AWS and EKS.","title":"EKS addons module"},{"location":"user-guides/eks-addons/#customization","text":"All the configuration is done in eks-addons/terragrunt.hcl . depe n de n cies { pa t hs = [ \"../eks-addons-critical\" ] } i n clude \"root\" { pa t h = f i n d_i n _pare nt _ f olders() expose = true merge_s trate gy = \"deep\" } i n clude \"vpc\" { pa t h = \"../../../../../../dependency-blocks/vpc.hcl\" expose = true merge_s trate gy = \"deep\" } i n clude \"eks\" { pa t h = \"../../../../../../dependency-blocks/eks.hcl\" expose = true merge_s trate gy = \"deep\" } terraf orm { source = \"github.com/particuleio/terraform-kubernetes-addons.git//modules/aws?ref=v5.0.0\" } ge nerate \"provider-local\" { pa t h = \"provider-local.tf\" i f _exis ts = \"overwrite\" co ntents = f ile( \"../../../../../../provider-config/eks-addons/eks-addons.tf\" ) } ge nerate \"provider-github\" { pa t h = \"provider-github.tf\" i f _exis ts = \"overwrite_terragrunt\" co ntents = << -E OF provider \"github\" { ow ner = \"${include.root.locals.merged.github_owner}\" } EOF } i n pu ts = { priori t y - class = { na me = base na me(ge t _ terra gru nt _dir()) } priori t y - class - ds = { na me = \"${basename(get_terragrunt_dir())}-ds\" } clus ter - na me = depe n de n cy.eks.ou t pu ts .clus ter _id ta gs = merge( i n clude.roo t .locals.cus t om_ ta gs ) eks = { \"cluster_oidc_issuer_url\" = depe n de n cy.eks.ou t pu ts .clus ter _oidc_issuer_url } cer t - ma na ger = { e na bled = true acme_h tt p 01 _e na bled = true acme_d ns 01 _e na bled = true acme_h tt p 01 _i n gress_class = \"nginx\" ex tra _values = << -E XTRA_VALUES i n gressShim : de fault IssuerName : le tsen cryp t de fault IssuerKi n d : Clus ter Issuer de fault IssuerGroup : cer t - ma na ger.io EXTRA_VALUES } clus ter - au t oscaler = { e na bled = true versio n = \"v1.21.2\" ex tra _values = << -E XTRA_VALUES ex tra Args : scale - dow n - u t iliza t io n - t hreshold : 0.7 EXTRA_VALUES } ex ternal - d ns = { ex ternal - d ns = { e na bled = true } } # For t his t o work : # * GITHUB_TOKEN should be se t flu x 2 = { e na bled = true tar ge t _pa t h = \"gitops/clusters/${include.root.locals.merged.env}/${include.root.locals.merged.name}\" gi t hub_url = \"ssh://git@github.com/particuleio/teks\" reposi t ory = \"teks\" bra n ch = \"flux\" reposi t ory_visibili t y = \"public\" versio n = \"v0.27.1\" au t o_image_upda te = true } i n gress - n gi n x = { e na bled = true use_ nl b_ip = true allowed_cidrs = depe n de n cy.vpc.ou t pu ts .priva te _sub nets _cidr_blocks ex tra _values = << -E XTRA_VALUES co ntr oller : i n gressClassResource : e na bled : true de fault : true replicaCou nt : 2 mi n Available : 1 ki n d : \"Deployment\" resources : reques ts : cpu : 100 m memory : 64 Mi EXTRA_VALUES } kube - prome t heus - s ta ck = { e na bled = true allowed_cidrs = depe n de n cy.vpc.ou t pu ts .priva te _sub nets _cidr_blocks t ha n os_sidecar_e na bled = true t ha n os_bucke t _ f orce_des tr oy = true ex tra _values = << -E XTRA_VALUES gra fana : image : ta g : 8.3.4 deployme nt S trate gy : t ype : Recrea te i n gress : a nn o tat io ns : kuber netes .io/ tls - acme : \"true\" i n gressClassName : n gi n x e na bled : true hos ts : - tele me tr y.$ { i n clude.roo t .locals.merged.de fault _domai n _ na me } tls : - secre t Name : $ { i n clude.roo t .locals.merged.de fault _domai n _ na me } hos ts : - tele me tr y.$ { i n clude.roo t .locals.merged.de fault _domai n _ na me } persis ten ce : e na bled : true accessModes : - ReadWri te O n ce size : 1 Gi prome t heus : prome t heusSpec : n odeSelec t or : kuber netes .io/arch : amd 64 scrapeI nter val : 60 s re tent io n : 2 d re tent io n Size : \"10GB\" ruleSelec t orNilUsesHelmValues : false serviceMo n i t orSelec t orNilUsesHelmValues : false podMo n i t orSelec t orNilUsesHelmValues : false probeSelec t orNilUsesHelmValues : false s t orageSpec : volumeClaimTempla te : spec : accessModes : [ \"ReadWriteOnce\" ] resources : reques ts : s t orage : 10 Gi resources : reques ts : cpu : 1 memory : 2 Gi limi ts : cpu : 2 memory : 2 Gi EXTRA_VALUES } loki - s ta ck = { e na bled = true bucke t _ f orce_des tr oy = true ex tra _values = << - VALUES resources : reques ts : cpu : 1 memory : 2 Gi limi ts : cpu : 2 memory : 4 Gi co nf ig : limi ts _co nf ig : i n ges t io n _ra te _mb : 320 i n ges t io n _burs t _size_mb : 512 max_s trea ms_per_user : 100000 chu n k_s t ore_co nf ig : max_look_back_period : 2160 h ta ble_ma na ger : re tent io n _dele tes _e na bled : true re tent io n _period : 2160 h i n gress : e na bled : true a nn o tat io ns : kuber netes .io/ tls - acme : \"true\" n gi n x.i n gress.kuber netes .io/au t h - tls - veri f y - clie nt : \"on\" n gi n x.i n gress.kuber netes .io/au t h - tls - secre t : \"telemetry/loki-ca\" hos ts : - hos t : logz.$ { i n clude.roo t .locals.merged.de fault _domai n _ na me } pa t hs : [ \"/\" ] tls : - secre t Name : logz.$ { i n clude.roo t .locals.merged.de fault _domai n _ na me } hos ts : - logz.$ { i n clude.roo t .locals.merged.de fault _domai n _ na me } VALUES bucke t _li fe cycle_rule = [ { id = \"log\" e na bled = true trans i t io n = [ { days = 14 s t orage_class = \"INTELLIGENT_TIERING\" }, ] expira t io n = { days = 30 } }, ] } prom ta il = { e na bled = true wai t = false } t ha n os = { e na bled = true bucke t _ f orce_des tr oy = true # Wai t i n g f or ARM suppor t h tt ps : //gi t hub.com/bi tna mi/char ts /issues/ 7305 ex tra _values = << -E XTRA_VALUES query : n odeSelec t or : kuber netes .io/arch : amd 64 queryFro nten d : n odeSelec t or : kuber netes .io/arch : amd 64 bucke t web : n odeSelec t or : kuber netes .io/arch : amd 64 compac t or : n odeSelec t or : kuber netes .io/arch : amd 64 s t orega te way : n odeSelec t or : kuber netes .io/arch : amd 64 ruler : n odeSelec t or : kuber netes .io/arch : amd 64 receive : n odeSelec t or : kuber netes .io/arch : amd 64 receiveDis tr ibu t or : n odeSelec t or : kuber netes .io/arch : amd 64 EXTRA_VALUES } }","title":"Customization"},{"location":"user-guides/eks-addons/#default-charts-values","text":"Some values are defined by default directly into the module. These can off course be overridden and or merged/replaced. You can find the defaults values in the upstream module . Eg. default values for cluster-autoscaler are in cluster-autoscaler.tf .","title":"Default charts values"},{"location":"user-guides/eks-addons/#overriding-helm-provider-values","text":"Helm provider have defaults values defined here : helm_de faults _de faults = { a t omic = false clea nu p_o n _ fa il = false depe n de n cy_upda te = false disable_crd_hooks = false disable_webhooks = false f orce_upda te = false recrea te _pods = false re n der_subchar t _ n o tes = true replace = false rese t _values = false reuse_values = false skip_crds = false t imeou t = 3600 veri f y = false wai t = true ex tra _values = \"\" } These can be overridden globally with the helm_defaults input variable or can be overridden per chart in terragrunt.hcl : helm_de faults = { replace = true veri f y = true t imeou t = 300 } clus ter _au t oscaler = { crea te _iam_resources_irsa = true iam_policy_override = \"\" versio n = \"v1.14.7\" char t _versio n = \"6.4.0\" e na bled = true de fault _ net work_policy = true clus ter _ na me = depe n de n cy.eks.ou t pu ts .clus ter _id t imeou t = 3600 <= here you ca n add a n y helm provider override }","title":"Overriding Helm provider values"},{"location":"user-guides/eks-addons/#overriding-charts-valuesyaml","text":"It is possible to add or override values per charts. Helm provider use the same merge logic as Helm so you can basically rewrite the whole values.yaml if needed. Each chart has a extra_values variable where you can specify custom values. flu x = { crea te _iam_resources_irsa = true versio n = \"1.18.0\" char t _versio n = \"1.2.0\" e na bled = false de fault _ net work_policy = true ex tra _values = <<EXTRA_VALUES gi t : url : \"ssh://git@gitlab.com/myrepo/gitops-${local.env}.git\" pollI nter val : \"2m\" rbac : crea te : false regis tr y : au t oma t io n I nter val : \"2m\" EXTRA_VALUES } There are some examples in the terragrunt.hcl file. Not all the variables available are present. If you want a full list of variable, you can find them in the upstream module . For example for cluster-autoscaler you can see the default here .","title":"Overriding charts values.yaml"},{"location":"user-guides/eks/","text":"EKS module Upstream configuration \u00b6 EKS module is also upstream and allow to deploy an EKS cluster which supports: managed node pools self managed node groups using launch template tEKS uses EKS managed node groups by default and use one node pool per availability zone. You can use any inputs from the upstream module to configure the cluster in eks/terragrunt.hcl . See all available feature here i n clude \"root\" { pa t h = f i n d_i n _pare nt _ f olders() expose = true merge_s trate gy = \"deep\" } i n clude \"vpc\" { pa t h = \"../../../../../../dependency-blocks/vpc.hcl\" expose = true merge_s trate gy = \"deep\" } i n clude \"encryption_config\" { pa t h = \"../../../../../../dependency-blocks/encryption-config.hcl\" expose = true merge_s trate gy = \"deep\" } terraf orm { source = \"github.com/terraform-aws-modules/terraform-aws-eks?ref=v18.17.0\" a fter _hook \"kubeconfig\" { comma n ds = [ \"apply\" ] execu te = [ \"bash\" , \"-c\" , \"aws eks update-kubeconfig --name ${include.root.locals.full_name} --kubeconfig ${get_terragrunt_dir()}/kubeconfig 2>/dev/null\" ] } a fter _hook \"kube-system-label\" { comma n ds = [ \"apply\" ] execu te = [ \"bash\" , \"-c\" , \"kubectl --kubeconfig ${get_terragrunt_dir()}/kubeconfig label ns kube-system name=kube-system --overwrite\" ] } a fter _hook \"undefault-gp2\" { comma n ds = [ \"apply\" ] execu te = [ \"bash\" , \"-c\" , \"kubectl --kubeconfig ${get_terragrunt_dir()}/kubeconfig patch storageclass gp2 -p '{\\\"metadata\\\": {\\\"annotations\\\":{\\\"storageclass.kubernetes.io/is-default-class\\\":\\\"false\\\"}}}'\" ] } a fter _hook \"vpc-cni-prefix-delegation\" { comma n ds = [ \"apply\" ] execu te = [ \"bash\" , \"-c\" , \"kubectl --kubeconfig ${get_terragrunt_dir()}/kubeconfig set env daemonset aws-node -n kube-system ENABLE_PREFIX_DELEGATION=true\" ] } a fter _hook \"vpc-cni-prefix-warm-prefix\" { comma n ds = [ \"apply\" ] execu te = [ \"bash\" , \"-c\" , \"kubectl --kubeconfig ${get_terragrunt_dir()}/kubeconfig set env daemonset aws-node -n kube-system WARM_PREFIX_TARGET=1\" ] } } ge nerate \"provider-local\" { pa t h = \"provider-local.tf\" i f _exis ts = \"overwrite\" co ntents = f ile( \"../../../../../../provider-config/eks/eks.tf\" ) } i n pu ts = { aws = { \"region\" = i n clude.roo t .locals.merged.aws_regio n } ta gs = merge( i n clude.roo t .locals.cus t om_ ta gs ) clus ter _ na me = i n clude.roo t .locals. full _ na me clus ter _versio n = \"1.21\" clus ter _e na bled_log_ t ypes = [ \"api\" , \"audit\" , \"authenticator\" , \"controllerManager\" , \"scheduler\" ] clus ter _e n dpoi nt _priva te _access = true clus ter _e n dpoi nt _public_access = true clus ter _e n cryp t io n _co nf ig = [ { provider_key_ar n = depe n de n cy.e n cryp t io n _co nf ig.ou t pu ts .ar n resources = [ \"secrets\" ] } ] clus ter _addo ns = { cored ns = { addo n _versio n = \"v1.8.4-eksbuild.1\" resolve_co nfl ic ts = \"OVERWRITE\" } kube - proxy = { addo n _versio n = \"v1.21.2-eksbuild.2\" resolve_co nfl ic ts = \"OVERWRITE\" } vpc - c n i = { addo n _versio n = \"v1.10.1-eksbuild.1\" resolve_co nfl ic ts = \"OVERWRITE\" } } vpc_id = depe n de n cy.vpc.ou t pu ts .vpc_id sub net _ids = depe n de n cy.vpc.ou t pu ts .priva te _sub nets e na ble_irsa = true cloudwa t ch_log_group_re tent io n _i n _days = 7 n ode_securi t y_group_addi t io nal _rules = { i n gress_sel f _all = { fr om_por t = 0 t o_por t = 0 pro t ocol = \"-1\" t ype = \"ingress\" sel f = true } i n gress_clus ter _all = { fr om_por t = 0 t o_por t = 0 pro t ocol = \"-1\" t ype = \"ingress\" source_clus ter _securi t y_group = true } i n gress_ n ode_por t _ t cp = { fr om_por t = 30000 t o_por t = 32767 pro t ocol = \"tcp\" t ype = \"ingress\" cidr_blocks = [ \"0.0.0.0/0\" ] ipv 6 _cidr_blocks = [ \"::/0\" ] } i n gress_ n ode_por t _udp = { fr om_por t = 30000 t o_por t = 32767 pro t ocol = \"udp\" t ype = \"ingress\" cidr_blocks = [ \"0.0.0.0/0\" ] ipv 6 _cidr_blocks = [ \"::/0\" ] } egress_all = { fr om_por t = 0 t o_por t = 0 pro t ocol = \"-1\" t ype = \"egress\" cidr_blocks = [ \"0.0.0.0/0\" ] ipv 6 _cidr_blocks = [ \"::/0\" ] } } eks_ma na ged_ n ode_group_de faults = { f orce_upda te _versio n = true desired_size = 1 mi n _size = 0 max_size = 10 ebs_op t imized = true capaci t y_ t ype = \"ON_DEMAND\" iam_role_addi t io nal _policies = [ \"arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\" ] block_device_mappi n gs = { roo t = { device_ na me = \"/dev/xvda\" ebs = { volume_size = 15 volume_ t ype = \"gp3\" } } } } eks_ma na ged_ n ode_groups = { \"default-a\" = { desired_size = 1 ami_ t ype = \"AL2_x86_64\" pla tf orm = \"linux\" i nstan ce_ t ypes = [ \"t3a.large\" ] sub net _ids = [ depe n de n cy.vpc.ou t pu ts .priva te _sub nets [ 0 ]] pre_boo tstra p_user_da ta = << -E OT #!/bi n /bash se t -e x ca t << -E OF > /e t c/pro f ile.d/boo tstra p.sh expor t CONTAINER_RUNTIME= \"containerd\" expor t USE_MAX_PODS= false expor t KUBELET_EXTRA_ARGS= \"--max-pods=${run_cmd(\" /bi n /sh \", \" - c \", \" ../../../../../../../ t ools/max - pods - calcula t or.sh -- i nstan ce - t ype t 3 a.large -- c n i - versio n 1.10.1 -- c n i - pre f ix - delega t io n -e na bled \")}\" EOF # Source ex tra e n viro n me nt variables i n boo tstra p scrip t sed - i '/^se t - o errexi t /a\\\\ ns ource /e t c/pro f ile.d/boo tstra p.sh' /e t c/eks/boo tstra p.sh cd / t mp sudo yum i nstall - y h tt ps : //s 3. amazo na ws.com/ec 2- dow nl oads - wi n dows/SSMAge nt /la test /li nu x_amd 64 /amazo n - ssm - age nt .rpm sudo sys te mc tl e na ble amazo n - ssm - age nt sudo sys te mc tl s tart amazo n - ssm - age nt EOT labels = { net work = \"private\" } } \"default-b\" = { ami_ t ype = \"BOTTLEROCKET_x86_64\" pla tf orm = \"bottlerocket\" i nstan ce_ t ypes = [ \"t3a.large\" ] sub net _ids = [ depe n de n cy.vpc.ou t pu ts .priva te _sub nets [ 1 ]] e na ble_boo tstra p_user_da ta = true boo tstra p_ex tra _args = << -E OT \"max-pods\" = $ { ru n _cmd( \"/bin/sh\" , \"-c\" , \"../../../../../../../tools/max-pods-calculator.sh --instance-type t3a.large --cni-version 1.10.1 --cni-prefix-delegation-enabled\" ) } EOT block_device_mappi n gs = { roo t = { device_ na me = \"/dev/xvda\" ebs = { volume_size = 2 volume_ t ype = \"gp3\" dele te _o n _ ter mi nat io n = true e n cryp te d = true kms_key_id = depe n de n cy.e n cryp t io n _co nf ig.ou t pu ts .ar n } } co nta i ners = { device_ na me = \"/dev/xvdb\" ebs = { volume_size = 20 volume_ t ype = \"gp3\" dele te _o n _ ter mi nat io n = true e n cryp te d = true kms_key_id = depe n de n cy.e n cryp t io n _co nf ig.ou t pu ts .ar n } } } labels = { net work = \"private\" } } \"default-c\" = { ami_ t ype = \"BOTTLEROCKET_x86_64\" pla tf orm = \"bottlerocket\" i nstan ce_ t ypes = [ \"t3a.large\" ] sub net _ids = [ depe n de n cy.vpc.ou t pu ts .priva te _sub nets [ 2 ]] e na ble_boo tstra p_user_da ta = true boo tstra p_ex tra _args = << -E OT \"max-pods\" = $ { ru n _cmd( \"/bin/sh\" , \"-c\" , \"../../../../../../../tools/max-pods-calculator.sh --instance-type t3a.large --cni-version 1.10.1 --cni-prefix-delegation-enabled\" ) } EOT block_device_mappi n gs = { roo t = { device_ na me = \"/dev/xvda\" ebs = { volume_size = 2 volume_ t ype = \"gp3\" dele te _o n _ ter mi nat io n = true e n cryp te d = true kms_key_id = depe n de n cy.e n cryp t io n _co nf ig.ou t pu ts .ar n } } co nta i ners = { device_ na me = \"/dev/xvdb\" ebs = { volume_size = 20 volume_ t ype = \"gp3\" dele te _o n _ ter mi nat io n = true e n cryp te d = true kms_key_id = depe n de n cy.e n cryp t io n _co nf ig.ou t pu ts .ar n } } } labels = { net work = \"private\" } } \"arm-a\" = { ami_ t ype = \"AL2_ARM_64\" i nstan ce_ t ypes = [ \"t4g.medium\" ] sub net _ids = [ depe n de n cy.vpc.ou t pu ts .priva te _sub nets [ 0 ]] pre_boo tstra p_user_da ta = << -E OT #!/bi n /bash se t -e x ca t << -E OF > /e t c/pro f ile.d/boo tstra p.sh expor t CONTAINER_RUNTIME= \"containerd\" expor t USE_MAX_PODS= false expor t KUBELET_EXTRA_ARGS= \"--max-pods=${run_cmd(\" /bi n /sh \", \" - c \", \" ../../../../../../../ t ools/max - pods - calcula t or.sh -- i nstan ce - t ype t 4 g.medium -- c n i - versio n 1.10.1 -- c n i - pre f ix - delega t io n -e na bled \")}\" EOF # Source ex tra e n viro n me nt variables i n boo tstra p scrip t sed - i '/^se t - o errexi t /a\\\\ ns ource /e t c/pro f ile.d/boo tstra p.sh' /e t c/eks/boo tstra p.sh cd / t mp sudo yum i nstall - y h tt ps : //s 3. amazo na ws.com/ec 2- dow nl oads - wi n dows/SSMAge nt /la test /li nu x_arm 64 /amazo n - ssm - age nt .rpm sudo sys te mc tl e na ble amazo n - ssm - age nt sudo sys te mc tl s tart amazo n - ssm - age nt EOT labels = { net work = \"private\" } } \"arm-b\" = { ami_ t ype = \"BOTTLEROCKET_ARM_64\" pla tf orm = \"bottlerocket\" i nstan ce_ t ypes = [ \"t4g.medium\" ] sub net _ids = [ depe n de n cy.vpc.ou t pu ts .priva te _sub nets [ 1 ]] e na ble_boo tstra p_user_da ta = true boo tstra p_ex tra _args = << -E OT \"max-pods\" = $ { ru n _cmd( \"/bin/sh\" , \"-c\" , \"../../../../../../../tools/max-pods-calculator.sh --instance-type t4g.medium --cni-version 1.10.1 --cni-prefix-delegation-enabled\" ) } EOT block_device_mappi n gs = { roo t = { device_ na me = \"/dev/xvda\" ebs = { volume_size = 2 volume_ t ype = \"gp3\" dele te _o n _ ter mi nat io n = true e n cryp te d = true kms_key_id = depe n de n cy.e n cryp t io n _co nf ig.ou t pu ts .ar n } } co nta i ners = { device_ na me = \"/dev/xvdb\" ebs = { volume_size = 20 volume_ t ype = \"gp3\" dele te _o n _ ter mi nat io n = true e n cryp te d = true kms_key_id = depe n de n cy.e n cryp t io n _co nf ig.ou t pu ts .ar n } } } labels = { net work = \"private\" } } \"arm-c\" = { ami_ t ype = \"BOTTLEROCKET_ARM_64\" pla tf orm = \"bottlerocket\" i nstan ce_ t ypes = [ \"t4g.medium\" ] sub net _ids = [ depe n de n cy.vpc.ou t pu ts .priva te _sub nets [ 2 ]] e na ble_boo tstra p_user_da ta = true boo tstra p_ex tra _args = << -E OT \"max-pods\" = $ { ru n _cmd( \"/bin/sh\" , \"-c\" , \"../../../../../../../tools/max-pods-calculator.sh --instance-type t4g.large --cni-version 1.10.1 --cni-prefix-delegation-enabled\" ) } EOT block_device_mappi n gs = { roo t = { device_ na me = \"/dev/xvda\" ebs = { volume_size = 2 volume_ t ype = \"gp3\" dele te _o n _ ter mi nat io n = true e n cryp te d = true kms_key_id = depe n de n cy.e n cryp t io n _co nf ig.ou t pu ts .ar n } } co nta i ners = { device_ na me = \"/dev/xvdb\" ebs = { volume_size = 20 volume_ t ype = \"gp3\" dele te _o n _ ter mi nat io n = true e n cryp te d = true kms_key_id = depe n de n cy.e n cryp t io n _co nf ig.ou t pu ts .ar n } } } labels = { net work = \"private\" } } } }","title":"EKS"},{"location":"user-guides/eks/#upstream-configuration","text":"EKS module is also upstream and allow to deploy an EKS cluster which supports: managed node pools self managed node groups using launch template tEKS uses EKS managed node groups by default and use one node pool per availability zone. You can use any inputs from the upstream module to configure the cluster in eks/terragrunt.hcl . See all available feature here i n clude \"root\" { pa t h = f i n d_i n _pare nt _ f olders() expose = true merge_s trate gy = \"deep\" } i n clude \"vpc\" { pa t h = \"../../../../../../dependency-blocks/vpc.hcl\" expose = true merge_s trate gy = \"deep\" } i n clude \"encryption_config\" { pa t h = \"../../../../../../dependency-blocks/encryption-config.hcl\" expose = true merge_s trate gy = \"deep\" } terraf orm { source = \"github.com/terraform-aws-modules/terraform-aws-eks?ref=v18.17.0\" a fter _hook \"kubeconfig\" { comma n ds = [ \"apply\" ] execu te = [ \"bash\" , \"-c\" , \"aws eks update-kubeconfig --name ${include.root.locals.full_name} --kubeconfig ${get_terragrunt_dir()}/kubeconfig 2>/dev/null\" ] } a fter _hook \"kube-system-label\" { comma n ds = [ \"apply\" ] execu te = [ \"bash\" , \"-c\" , \"kubectl --kubeconfig ${get_terragrunt_dir()}/kubeconfig label ns kube-system name=kube-system --overwrite\" ] } a fter _hook \"undefault-gp2\" { comma n ds = [ \"apply\" ] execu te = [ \"bash\" , \"-c\" , \"kubectl --kubeconfig ${get_terragrunt_dir()}/kubeconfig patch storageclass gp2 -p '{\\\"metadata\\\": {\\\"annotations\\\":{\\\"storageclass.kubernetes.io/is-default-class\\\":\\\"false\\\"}}}'\" ] } a fter _hook \"vpc-cni-prefix-delegation\" { comma n ds = [ \"apply\" ] execu te = [ \"bash\" , \"-c\" , \"kubectl --kubeconfig ${get_terragrunt_dir()}/kubeconfig set env daemonset aws-node -n kube-system ENABLE_PREFIX_DELEGATION=true\" ] } a fter _hook \"vpc-cni-prefix-warm-prefix\" { comma n ds = [ \"apply\" ] execu te = [ \"bash\" , \"-c\" , \"kubectl --kubeconfig ${get_terragrunt_dir()}/kubeconfig set env daemonset aws-node -n kube-system WARM_PREFIX_TARGET=1\" ] } } ge nerate \"provider-local\" { pa t h = \"provider-local.tf\" i f _exis ts = \"overwrite\" co ntents = f ile( \"../../../../../../provider-config/eks/eks.tf\" ) } i n pu ts = { aws = { \"region\" = i n clude.roo t .locals.merged.aws_regio n } ta gs = merge( i n clude.roo t .locals.cus t om_ ta gs ) clus ter _ na me = i n clude.roo t .locals. full _ na me clus ter _versio n = \"1.21\" clus ter _e na bled_log_ t ypes = [ \"api\" , \"audit\" , \"authenticator\" , \"controllerManager\" , \"scheduler\" ] clus ter _e n dpoi nt _priva te _access = true clus ter _e n dpoi nt _public_access = true clus ter _e n cryp t io n _co nf ig = [ { provider_key_ar n = depe n de n cy.e n cryp t io n _co nf ig.ou t pu ts .ar n resources = [ \"secrets\" ] } ] clus ter _addo ns = { cored ns = { addo n _versio n = \"v1.8.4-eksbuild.1\" resolve_co nfl ic ts = \"OVERWRITE\" } kube - proxy = { addo n _versio n = \"v1.21.2-eksbuild.2\" resolve_co nfl ic ts = \"OVERWRITE\" } vpc - c n i = { addo n _versio n = \"v1.10.1-eksbuild.1\" resolve_co nfl ic ts = \"OVERWRITE\" } } vpc_id = depe n de n cy.vpc.ou t pu ts .vpc_id sub net _ids = depe n de n cy.vpc.ou t pu ts .priva te _sub nets e na ble_irsa = true cloudwa t ch_log_group_re tent io n _i n _days = 7 n ode_securi t y_group_addi t io nal _rules = { i n gress_sel f _all = { fr om_por t = 0 t o_por t = 0 pro t ocol = \"-1\" t ype = \"ingress\" sel f = true } i n gress_clus ter _all = { fr om_por t = 0 t o_por t = 0 pro t ocol = \"-1\" t ype = \"ingress\" source_clus ter _securi t y_group = true } i n gress_ n ode_por t _ t cp = { fr om_por t = 30000 t o_por t = 32767 pro t ocol = \"tcp\" t ype = \"ingress\" cidr_blocks = [ \"0.0.0.0/0\" ] ipv 6 _cidr_blocks = [ \"::/0\" ] } i n gress_ n ode_por t _udp = { fr om_por t = 30000 t o_por t = 32767 pro t ocol = \"udp\" t ype = \"ingress\" cidr_blocks = [ \"0.0.0.0/0\" ] ipv 6 _cidr_blocks = [ \"::/0\" ] } egress_all = { fr om_por t = 0 t o_por t = 0 pro t ocol = \"-1\" t ype = \"egress\" cidr_blocks = [ \"0.0.0.0/0\" ] ipv 6 _cidr_blocks = [ \"::/0\" ] } } eks_ma na ged_ n ode_group_de faults = { f orce_upda te _versio n = true desired_size = 1 mi n _size = 0 max_size = 10 ebs_op t imized = true capaci t y_ t ype = \"ON_DEMAND\" iam_role_addi t io nal _policies = [ \"arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\" ] block_device_mappi n gs = { roo t = { device_ na me = \"/dev/xvda\" ebs = { volume_size = 15 volume_ t ype = \"gp3\" } } } } eks_ma na ged_ n ode_groups = { \"default-a\" = { desired_size = 1 ami_ t ype = \"AL2_x86_64\" pla tf orm = \"linux\" i nstan ce_ t ypes = [ \"t3a.large\" ] sub net _ids = [ depe n de n cy.vpc.ou t pu ts .priva te _sub nets [ 0 ]] pre_boo tstra p_user_da ta = << -E OT #!/bi n /bash se t -e x ca t << -E OF > /e t c/pro f ile.d/boo tstra p.sh expor t CONTAINER_RUNTIME= \"containerd\" expor t USE_MAX_PODS= false expor t KUBELET_EXTRA_ARGS= \"--max-pods=${run_cmd(\" /bi n /sh \", \" - c \", \" ../../../../../../../ t ools/max - pods - calcula t or.sh -- i nstan ce - t ype t 3 a.large -- c n i - versio n 1.10.1 -- c n i - pre f ix - delega t io n -e na bled \")}\" EOF # Source ex tra e n viro n me nt variables i n boo tstra p scrip t sed - i '/^se t - o errexi t /a\\\\ ns ource /e t c/pro f ile.d/boo tstra p.sh' /e t c/eks/boo tstra p.sh cd / t mp sudo yum i nstall - y h tt ps : //s 3. amazo na ws.com/ec 2- dow nl oads - wi n dows/SSMAge nt /la test /li nu x_amd 64 /amazo n - ssm - age nt .rpm sudo sys te mc tl e na ble amazo n - ssm - age nt sudo sys te mc tl s tart amazo n - ssm - age nt EOT labels = { net work = \"private\" } } \"default-b\" = { ami_ t ype = \"BOTTLEROCKET_x86_64\" pla tf orm = \"bottlerocket\" i nstan ce_ t ypes = [ \"t3a.large\" ] sub net _ids = [ depe n de n cy.vpc.ou t pu ts .priva te _sub nets [ 1 ]] e na ble_boo tstra p_user_da ta = true boo tstra p_ex tra _args = << -E OT \"max-pods\" = $ { ru n _cmd( \"/bin/sh\" , \"-c\" , \"../../../../../../../tools/max-pods-calculator.sh --instance-type t3a.large --cni-version 1.10.1 --cni-prefix-delegation-enabled\" ) } EOT block_device_mappi n gs = { roo t = { device_ na me = \"/dev/xvda\" ebs = { volume_size = 2 volume_ t ype = \"gp3\" dele te _o n _ ter mi nat io n = true e n cryp te d = true kms_key_id = depe n de n cy.e n cryp t io n _co nf ig.ou t pu ts .ar n } } co nta i ners = { device_ na me = \"/dev/xvdb\" ebs = { volume_size = 20 volume_ t ype = \"gp3\" dele te _o n _ ter mi nat io n = true e n cryp te d = true kms_key_id = depe n de n cy.e n cryp t io n _co nf ig.ou t pu ts .ar n } } } labels = { net work = \"private\" } } \"default-c\" = { ami_ t ype = \"BOTTLEROCKET_x86_64\" pla tf orm = \"bottlerocket\" i nstan ce_ t ypes = [ \"t3a.large\" ] sub net _ids = [ depe n de n cy.vpc.ou t pu ts .priva te _sub nets [ 2 ]] e na ble_boo tstra p_user_da ta = true boo tstra p_ex tra _args = << -E OT \"max-pods\" = $ { ru n _cmd( \"/bin/sh\" , \"-c\" , \"../../../../../../../tools/max-pods-calculator.sh --instance-type t3a.large --cni-version 1.10.1 --cni-prefix-delegation-enabled\" ) } EOT block_device_mappi n gs = { roo t = { device_ na me = \"/dev/xvda\" ebs = { volume_size = 2 volume_ t ype = \"gp3\" dele te _o n _ ter mi nat io n = true e n cryp te d = true kms_key_id = depe n de n cy.e n cryp t io n _co nf ig.ou t pu ts .ar n } } co nta i ners = { device_ na me = \"/dev/xvdb\" ebs = { volume_size = 20 volume_ t ype = \"gp3\" dele te _o n _ ter mi nat io n = true e n cryp te d = true kms_key_id = depe n de n cy.e n cryp t io n _co nf ig.ou t pu ts .ar n } } } labels = { net work = \"private\" } } \"arm-a\" = { ami_ t ype = \"AL2_ARM_64\" i nstan ce_ t ypes = [ \"t4g.medium\" ] sub net _ids = [ depe n de n cy.vpc.ou t pu ts .priva te _sub nets [ 0 ]] pre_boo tstra p_user_da ta = << -E OT #!/bi n /bash se t -e x ca t << -E OF > /e t c/pro f ile.d/boo tstra p.sh expor t CONTAINER_RUNTIME= \"containerd\" expor t USE_MAX_PODS= false expor t KUBELET_EXTRA_ARGS= \"--max-pods=${run_cmd(\" /bi n /sh \", \" - c \", \" ../../../../../../../ t ools/max - pods - calcula t or.sh -- i nstan ce - t ype t 4 g.medium -- c n i - versio n 1.10.1 -- c n i - pre f ix - delega t io n -e na bled \")}\" EOF # Source ex tra e n viro n me nt variables i n boo tstra p scrip t sed - i '/^se t - o errexi t /a\\\\ ns ource /e t c/pro f ile.d/boo tstra p.sh' /e t c/eks/boo tstra p.sh cd / t mp sudo yum i nstall - y h tt ps : //s 3. amazo na ws.com/ec 2- dow nl oads - wi n dows/SSMAge nt /la test /li nu x_arm 64 /amazo n - ssm - age nt .rpm sudo sys te mc tl e na ble amazo n - ssm - age nt sudo sys te mc tl s tart amazo n - ssm - age nt EOT labels = { net work = \"private\" } } \"arm-b\" = { ami_ t ype = \"BOTTLEROCKET_ARM_64\" pla tf orm = \"bottlerocket\" i nstan ce_ t ypes = [ \"t4g.medium\" ] sub net _ids = [ depe n de n cy.vpc.ou t pu ts .priva te _sub nets [ 1 ]] e na ble_boo tstra p_user_da ta = true boo tstra p_ex tra _args = << -E OT \"max-pods\" = $ { ru n _cmd( \"/bin/sh\" , \"-c\" , \"../../../../../../../tools/max-pods-calculator.sh --instance-type t4g.medium --cni-version 1.10.1 --cni-prefix-delegation-enabled\" ) } EOT block_device_mappi n gs = { roo t = { device_ na me = \"/dev/xvda\" ebs = { volume_size = 2 volume_ t ype = \"gp3\" dele te _o n _ ter mi nat io n = true e n cryp te d = true kms_key_id = depe n de n cy.e n cryp t io n _co nf ig.ou t pu ts .ar n } } co nta i ners = { device_ na me = \"/dev/xvdb\" ebs = { volume_size = 20 volume_ t ype = \"gp3\" dele te _o n _ ter mi nat io n = true e n cryp te d = true kms_key_id = depe n de n cy.e n cryp t io n _co nf ig.ou t pu ts .ar n } } } labels = { net work = \"private\" } } \"arm-c\" = { ami_ t ype = \"BOTTLEROCKET_ARM_64\" pla tf orm = \"bottlerocket\" i nstan ce_ t ypes = [ \"t4g.medium\" ] sub net _ids = [ depe n de n cy.vpc.ou t pu ts .priva te _sub nets [ 2 ]] e na ble_boo tstra p_user_da ta = true boo tstra p_ex tra _args = << -E OT \"max-pods\" = $ { ru n _cmd( \"/bin/sh\" , \"-c\" , \"../../../../../../../tools/max-pods-calculator.sh --instance-type t4g.large --cni-version 1.10.1 --cni-prefix-delegation-enabled\" ) } EOT block_device_mappi n gs = { roo t = { device_ na me = \"/dev/xvda\" ebs = { volume_size = 2 volume_ t ype = \"gp3\" dele te _o n _ ter mi nat io n = true e n cryp te d = true kms_key_id = depe n de n cy.e n cryp t io n _co nf ig.ou t pu ts .ar n } } co nta i ners = { device_ na me = \"/dev/xvdb\" ebs = { volume_size = 20 volume_ t ype = \"gp3\" dele te _o n _ ter mi nat io n = true e n cryp te d = true kms_key_id = depe n de n cy.e n cryp t io n _co nf ig.ou t pu ts .ar n } } } labels = { net work = \"private\" } } } }","title":"Upstream configuration"},{"location":"user-guides/getting-started/","text":"Getting started \u00b6 Tooling requirements \u00b6 The necessary tools are in requirements.yaml you can install them any way you want, make sure they are available in your $PATH. The following dependencies are required on the deployer host: Terraform Terragrunt kubectl helm aws-iam-authenticator AWS requirements \u00b6 At least one AWS account awscli configured ( see installation instructions ) to access your AWS account. A route53 hosted zone if you plan to use external-dns or cert-manager but it is not a hard requirement. Getting the template repository \u00b6 You can either clone the repo locally or generate/fork a template from github. git clone https://github.com/particuleio/teks.git The terraform directory structure is the following: . \u2514\u2500\u2500 live \u251c\u2500\u2500 backend \u2502 \u251c\u2500\u2500 backend.tf \u2502 \u251c\u2500\u2500 providers.tf \u2502 \u2514\u2500\u2500 state.tf \u251c\u2500\u2500 demo \u2502 \u251c\u2500\u2500 env_tags.yaml \u2502 \u2514\u2500\u2500 eu-west-3 \u2502 \u251c\u2500\u2500 clusters \u2502 \u2502 \u2514\u2500\u2500 full \u2502 \u2502 \u251c\u2500\u2500 eks \u2502 \u2502 \u2502 \u251c\u2500\u2500 aws-provider.tf -> ../../../../../shared/aws-provider.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 backend.tf -> ../../../../../backend/backend.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 data.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 locals.tf -> ../../../../../shared/locals.tf \u2502 \u2502 \u2502 \u2514\u2500\u2500 main.tf \u2502 \u2502 \u251c\u2500\u2500 eks-addons \u2502 \u2502 \u2502 \u251c\u2500\u2500 aws-provider.tf -> ../../../../../shared/aws-provider.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 backend.tf -> ../../../../../backend/backend.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 data.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 locals.tf -> ../../../../../shared/locals.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 main.tf \u2502 \u2502 \u2502 \u2514\u2500\u2500 versions.tf \u2502 \u2502 \u2514\u2500\u2500 vpc \u2502 \u2502 \u251c\u2500\u2500 aws-provider.tf -> ../../../../../shared/aws-provider.tf \u2502 \u2502 \u251c\u2500\u2500 backend.tf -> ../../../../../backend/backend.tf \u2502 \u2502 \u251c\u2500\u2500 locals.tf -> ../../../../../shared/locals.tf \u2502 \u2502 \u2514\u2500\u2500 main.tf \u2502 \u2514\u2500\u2500 region_values.yaml \u251c\u2500\u2500 global_tags.yaml \u251c\u2500\u2500 global_values.yaml \u2514\u2500\u2500 shared \u251c\u2500\u2500 aws-provider.tf \u2514\u2500\u2500 locals.tf Each cluster in inside the terraform/live folder and then modules are grouped by AWS region. Start a new cluster \u00b6 Create a new cluster beside demo : cp -ar demo mycluster Configuring the remote state \u00b6 Configuration of the remote state is based on the value of the global_values.yaml file in the terraform and the terragrunt directories based on the installation method you used. Both files are following the same structure. --- # Your AWS Account ID aws_account_id : 161285725140 # Prefix to be added to created resources prefix : pio-teks # AWS S3 bucket region where Terraform will store state tf_state_bucket_region : eu-west-1 # Github username or organization, this can be used by Flux2 to auto configure # Github bootstrap github_owner : particuleio Adapt these values to match your configuration ( prefix , 'project'). Based on the configuration, both methods will create the following resources: S3 bucket named {prefix}-{project}-{tf|tg}-state : store the state DynamoDB table named {prefix}-{project}-{tf|tg}-state-lock : prevent concurrent use The resource names will include information based on the configuration method used. Using terraform will create resources with tf in their name, and tg when using terragrunt . Using the current values, the resources created to use terraform will be: S3: pio-teks-tf-state DynamoDB: pio-state-tf-state-lock Remote state for Terraform \u00b6 If you plan on using terraform to setup teks , you need to create your remote backend using cloudposse/terraform-aws-tfstate-backend configured in terraform/live/backend/state.tf . In order to configure the S3 backend for terraform, configure your global_values.yaml then go in the terraform/live/backend directory. terraform init init the terraform module. terraform apply -auto-approve to create the S3 backend. terraform init -force-copy will copy the local backend to the S3 backend. The terraform-aws-tfstate-backend module will create or update the terraform/live/backend/backend.tf file, which is symlinked to the child modules ( vpc , eks , eks-addons ). Further documentation regarding the remote backend configuration can be found at terraform-aws-tfstate-backend#create . Remote state for Terragrunt \u00b6 terragrunt/live/demo/terragrunt.hcl is the parent terragrunt file use to configure remote state. The configuration is done automatically based on the terragrunt/live/global_values.yaml file. The values here will generate automatically the parent terragrunt file. You can either customize the values or edit directly the terragrunt.hcl file. Running Terragrunt command \u00b6 Terragrunt command are run inside their respective folder, for example, to run the vpc module: cd vpc terragrunt apply","title":"Getting started"},{"location":"user-guides/getting-started/#getting-started","text":"","title":"Getting started"},{"location":"user-guides/getting-started/#tooling-requirements","text":"The necessary tools are in requirements.yaml you can install them any way you want, make sure they are available in your $PATH. The following dependencies are required on the deployer host: Terraform Terragrunt kubectl helm aws-iam-authenticator","title":"Tooling requirements"},{"location":"user-guides/getting-started/#aws-requirements","text":"At least one AWS account awscli configured ( see installation instructions ) to access your AWS account. A route53 hosted zone if you plan to use external-dns or cert-manager but it is not a hard requirement.","title":"AWS requirements"},{"location":"user-guides/getting-started/#getting-the-template-repository","text":"You can either clone the repo locally or generate/fork a template from github. git clone https://github.com/particuleio/teks.git The terraform directory structure is the following: . \u2514\u2500\u2500 live \u251c\u2500\u2500 backend \u2502 \u251c\u2500\u2500 backend.tf \u2502 \u251c\u2500\u2500 providers.tf \u2502 \u2514\u2500\u2500 state.tf \u251c\u2500\u2500 demo \u2502 \u251c\u2500\u2500 env_tags.yaml \u2502 \u2514\u2500\u2500 eu-west-3 \u2502 \u251c\u2500\u2500 clusters \u2502 \u2502 \u2514\u2500\u2500 full \u2502 \u2502 \u251c\u2500\u2500 eks \u2502 \u2502 \u2502 \u251c\u2500\u2500 aws-provider.tf -> ../../../../../shared/aws-provider.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 backend.tf -> ../../../../../backend/backend.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 data.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 locals.tf -> ../../../../../shared/locals.tf \u2502 \u2502 \u2502 \u2514\u2500\u2500 main.tf \u2502 \u2502 \u251c\u2500\u2500 eks-addons \u2502 \u2502 \u2502 \u251c\u2500\u2500 aws-provider.tf -> ../../../../../shared/aws-provider.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 backend.tf -> ../../../../../backend/backend.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 data.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 locals.tf -> ../../../../../shared/locals.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 main.tf \u2502 \u2502 \u2502 \u2514\u2500\u2500 versions.tf \u2502 \u2502 \u2514\u2500\u2500 vpc \u2502 \u2502 \u251c\u2500\u2500 aws-provider.tf -> ../../../../../shared/aws-provider.tf \u2502 \u2502 \u251c\u2500\u2500 backend.tf -> ../../../../../backend/backend.tf \u2502 \u2502 \u251c\u2500\u2500 locals.tf -> ../../../../../shared/locals.tf \u2502 \u2502 \u2514\u2500\u2500 main.tf \u2502 \u2514\u2500\u2500 region_values.yaml \u251c\u2500\u2500 global_tags.yaml \u251c\u2500\u2500 global_values.yaml \u2514\u2500\u2500 shared \u251c\u2500\u2500 aws-provider.tf \u2514\u2500\u2500 locals.tf Each cluster in inside the terraform/live folder and then modules are grouped by AWS region.","title":"Getting the template repository"},{"location":"user-guides/getting-started/#start-a-new-cluster","text":"Create a new cluster beside demo : cp -ar demo mycluster","title":"Start a new cluster"},{"location":"user-guides/getting-started/#configuring-the-remote-state","text":"Configuration of the remote state is based on the value of the global_values.yaml file in the terraform and the terragrunt directories based on the installation method you used. Both files are following the same structure. --- # Your AWS Account ID aws_account_id : 161285725140 # Prefix to be added to created resources prefix : pio-teks # AWS S3 bucket region where Terraform will store state tf_state_bucket_region : eu-west-1 # Github username or organization, this can be used by Flux2 to auto configure # Github bootstrap github_owner : particuleio Adapt these values to match your configuration ( prefix , 'project'). Based on the configuration, both methods will create the following resources: S3 bucket named {prefix}-{project}-{tf|tg}-state : store the state DynamoDB table named {prefix}-{project}-{tf|tg}-state-lock : prevent concurrent use The resource names will include information based on the configuration method used. Using terraform will create resources with tf in their name, and tg when using terragrunt . Using the current values, the resources created to use terraform will be: S3: pio-teks-tf-state DynamoDB: pio-state-tf-state-lock","title":"Configuring the remote state"},{"location":"user-guides/getting-started/#remote-state-for-terraform","text":"If you plan on using terraform to setup teks , you need to create your remote backend using cloudposse/terraform-aws-tfstate-backend configured in terraform/live/backend/state.tf . In order to configure the S3 backend for terraform, configure your global_values.yaml then go in the terraform/live/backend directory. terraform init init the terraform module. terraform apply -auto-approve to create the S3 backend. terraform init -force-copy will copy the local backend to the S3 backend. The terraform-aws-tfstate-backend module will create or update the terraform/live/backend/backend.tf file, which is symlinked to the child modules ( vpc , eks , eks-addons ). Further documentation regarding the remote backend configuration can be found at terraform-aws-tfstate-backend#create .","title":"Remote state for Terraform"},{"location":"user-guides/getting-started/#remote-state-for-terragrunt","text":"terragrunt/live/demo/terragrunt.hcl is the parent terragrunt file use to configure remote state. The configuration is done automatically based on the terragrunt/live/global_values.yaml file. The values here will generate automatically the parent terragrunt file. You can either customize the values or edit directly the terragrunt.hcl file.","title":"Remote state for Terragrunt"},{"location":"user-guides/getting-started/#running-terragrunt-command","text":"Terragrunt command are run inside their respective folder, for example, to run the vpc module: cd vpc terragrunt apply","title":"Running Terragrunt command"},{"location":"user-guides/vpc/","text":"VPC module \u00b6 The vpc module is the one from upstream . To customize it. Modify the vpc/terragrunt.hcl file. You can use any inputs available in the upstream module. i n clude \"root\" { pa t h = f i n d_i n _pare nt _ f olders() expose = true merge_s trate gy = \"deep\" } terraf orm { source = \"github.com/terraform-aws-modules/terraform-aws-vpc?ref=v3.14.0\" } locals { azs = [ \"${include.root.locals.merged.aws_region}a\" , \"${include.root.locals.merged.aws_region}b\" , \"${include.root.locals.merged.aws_region}c\" ] cidr = \"10.0.0.0/16\" sub nets = cidrsub nets (local.cidr , 3 , 3 , 3 , 3 , 3 , 3 ) priva te _sub nets = chu n klis t (local.sub nets , 3 ) [ 0 ] public_sub nets = chu n klis t (local.sub nets , 3 ) [ 1 ] } i n pu ts = { ta gs = merge( i n clude.roo t .locals.cus t om_ ta gs ) na me = i n clude.roo t .locals. full _ na me cidr = local.cidr azs = local.azs priva te _sub nets = local.priva te _sub nets public_sub nets = local.public_sub nets e na ble_ipv 6 = true assig n _ipv 6 _address_o n _crea t io n = true public_sub net _ipv 6 _pre f ixes = [ 0 , 1 , 2 ] priva te _sub net _ipv 6 _pre f ixes = [ 3 , 4 , 5 ] e na ble_ nat _ga te way = true si n gle_ nat _ga te way = true e na ble_d ns _hos tna mes = true e na ble_d ns _suppor t = true public_sub net _ ta gs = { \"kubernetes.io/cluster/${include.root.locals.full_name}\" = \"shared\" \"kubernetes.io/role/elb\" = \"1\" } priva te _sub net _ ta gs = { \"kubernetes.io/cluster/${include.root.locals.full_name}\" = \"shared\" \"kubernetes.io/role/internal-elb\" = \"1\" } }","title":"VPC"},{"location":"user-guides/vpc/#vpc-module","text":"The vpc module is the one from upstream . To customize it. Modify the vpc/terragrunt.hcl file. You can use any inputs available in the upstream module. i n clude \"root\" { pa t h = f i n d_i n _pare nt _ f olders() expose = true merge_s trate gy = \"deep\" } terraf orm { source = \"github.com/terraform-aws-modules/terraform-aws-vpc?ref=v3.14.0\" } locals { azs = [ \"${include.root.locals.merged.aws_region}a\" , \"${include.root.locals.merged.aws_region}b\" , \"${include.root.locals.merged.aws_region}c\" ] cidr = \"10.0.0.0/16\" sub nets = cidrsub nets (local.cidr , 3 , 3 , 3 , 3 , 3 , 3 ) priva te _sub nets = chu n klis t (local.sub nets , 3 ) [ 0 ] public_sub nets = chu n klis t (local.sub nets , 3 ) [ 1 ] } i n pu ts = { ta gs = merge( i n clude.roo t .locals.cus t om_ ta gs ) na me = i n clude.roo t .locals. full _ na me cidr = local.cidr azs = local.azs priva te _sub nets = local.priva te _sub nets public_sub nets = local.public_sub nets e na ble_ipv 6 = true assig n _ipv 6 _address_o n _crea t io n = true public_sub net _ipv 6 _pre f ixes = [ 0 , 1 , 2 ] priva te _sub net _ipv 6 _pre f ixes = [ 3 , 4 , 5 ] e na ble_ nat _ga te way = true si n gle_ nat _ga te way = true e na ble_d ns _hos tna mes = true e na ble_d ns _suppor t = true public_sub net _ ta gs = { \"kubernetes.io/cluster/${include.root.locals.full_name}\" = \"shared\" \"kubernetes.io/role/elb\" = \"1\" } priva te _sub net _ ta gs = { \"kubernetes.io/cluster/${include.root.locals.full_name}\" = \"shared\" \"kubernetes.io/role/internal-elb\" = \"1\" } }","title":"VPC module"}]}